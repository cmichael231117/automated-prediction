{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the data using the file path\n",
    "train_filepath = os.sep.join(data_path + ['Orange_Telecom_Churn_Data_train.csv'])\n",
    "test_filepath = os.sep.join(data_path + ['Orange_Telecom_Churn_Data_test.csv'])\n",
    "\n",
    "# csv to pandas DataFrame\n",
    "train_data = pd.read_csv(train_filepath)\n",
    "test_data = pd.read_csv(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use 5 features\n",
    "\n",
    "train_data.drop(['state', 'area_code', 'account_length', 'total_day_calls', 'phone_number', 'total_day_minutes', 'total_eve_minutes', 'total_eve_calls', 'total_night_minutes', 'total_night_calls', 'total_intl_minutes', 'total_intl_calls', 'total_intl_charge', 'number_vmail_messages', 'total_night_charge'], axis=1, inplace=True)\n",
    "\n",
    "test_data.drop(['state', 'area_code', 'account_length', 'total_day_calls', 'phone_number', 'total_day_minutes', 'total_eve_minutes', 'total_eve_calls', 'total_night_minutes', 'total_night_calls', 'total_intl_minutes', 'total_intl_calls', 'total_intl_charge', 'number_vmail_messages', 'total_night_charge'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['intl_plan', 'voice_mail_plan', 'total_day_charge', 'total_eve_charge',\n       'number_customer_service_calls', 'churned'],\n      dtype='object')"
     },
     "execution_count": 5284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - label feature to number\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "for col in ['intl_plan', 'voice_mail_plan', 'churned']:\n",
    "    train_data[col] = lb.fit_transform(train_data[col])\n",
    "\n",
    "for col in ['intl_plan', 'voice_mail_plan', 'churned']:\n",
    "    test_data[col] = lb.fit_transform(test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - scale\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "train_data = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)\n",
    "\n",
    "test_data = pd.DataFrame(scaler.fit_transform(test_data), columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate stage1/stage2 data from train data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = train_data.copy()\n",
    "y_train = X_train.pop('churned')\n",
    "\n",
    "X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(X_train, y_train, test_size=0.45, stratify=y_train)\n",
    "\n",
    "train_stage1_data = pd.concat([X_train_stage1, y_train_stage1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi undersampling and create undersampled data set\n",
    "\n",
    "import numpy as np\n",
    "import math as math\n",
    "\n",
    "non_churend_indices = train_stage1_data[train_stage1_data.churned == 0].index\n",
    "non_churend_indices = np.random.permutation(non_churend_indices)    # shuffle indices\n",
    "\n",
    "churned_indices = train_stage1_data[train_stage1_data.churned == 1].index\n",
    "\n",
    "# calculate count of the undersampled data set\n",
    "sample_count = math.floor(len(non_churend_indices) / len(churned_indices))\n",
    "\n",
    "non_churned_indices_set = np.array_split(non_churend_indices, sample_count)\n",
    "\n",
    "train_stage1_samples = []\n",
    "# pair each non churned data with churned data\n",
    "for i in range(sample_count):\n",
    "    train_stage1_samples.append(\n",
    "        pd.concat([\n",
    "            train_stage1_data.loc[churned_indices],\n",
    "            train_stage1_data.loc[non_churned_indices_set[i]]\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage1_set = []\n",
    "y_train_stage1_set = []\n",
    "\n",
    "for i in range(sample_count):\n",
    "    X_train_stage1_set.append(train_stage1_samples[i].copy())\n",
    "    y_train_stage1_set.append(X_train_stage1_set[i].pop('churned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and learning knn model for each undersampled data\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_set_stage1 = []\n",
    "for i in range(sample_count):\n",
    "    knn_set_stage1.append(KNeighborsClassifier(n_neighbors=25))\n",
    "    knn_set_stage1[i].fit(X_train_stage1_set[i], y_train_stage1_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "y_pred_set_stage1 = []\n",
    "for i in range(sample_count):\n",
    "    y_pred_set_stage1.append(knn_set_stage1[i].predict(X_test_stage1))\n",
    "    \n",
    "total_y_pred_stage1 = np.sum(y_pred_set_stage1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting\n",
    "\n",
    "y_pred_stage1 = []\n",
    "for i in range(len(total_y_pred_stage1)): \n",
    "    if (total_y_pred_stage1[i] >= 2):\n",
    "        y_pred_stage1.append(1)\n",
    "    else:\n",
    "        y_pred_stage1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data which are predicted as true\n",
    "\n",
    "pred_true_indices = [i for i, y in enumerate(y_pred_stage1) if y == 1]\n",
    "pred_true_indices = pd.Index(pred_true_indices)\n",
    "\n",
    "X_train_stage2 = X_test_stage1.copy().reset_index()\n",
    "X_train_stage2.drop(['index'], axis=1, inplace=True)\n",
    "X_train_stage2 = X_train_stage2.loc[pred_true_indices]\n",
    "\n",
    "y_train_stage2 = y_test_stage1.copy().reset_index()\n",
    "y_train_stage2.drop(['index'], axis=1, inplace=True)\n",
    "y_train_stage2 = y_train_stage2.loc[pred_true_indices].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7209302325581395"
     },
     "execution_count": 5294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create stage2 knn model and learning using the data which are predicted as true at stage1\n",
    "\n",
    "knn_stage2 = KNeighborsClassifier(n_neighbors=25)\n",
    "knn_stage2.fit(X_train_stage2, y_train_stage2)\n",
    "\n",
    "y_test_train_stage2 = knn_stage2.predict(X_train_stage2)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "accuracy = metrics.accuracy_score(y_train_stage2, y_test_train_stage2)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5295,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### real predict #####\n",
    "# ready for predicting test data\n",
    "\n",
    "X_test = test_data.copy()\n",
    "y_test = X_test.pop('churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using stage1 knn models\n",
    "\n",
    "y_pred_set_for_test = []\n",
    "for i in range(sample_count):\n",
    "    y_pred_set_for_test.append(knn_set_stage1[i].predict(X_test))\n",
    "    \n",
    "total_y_pred_for_test = np.sum(y_pred_set_for_test, axis=0)\n",
    "\n",
    "# voting\n",
    "\n",
    "y_pred_for_test = []\n",
    "for i in range(len(total_y_pred_for_test)): \n",
    "    if (total_y_pred_for_test[i] >= 2):\n",
    "        y_pred_for_test.append(1)\n",
    "    else:\n",
    "        y_pred_for_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.886983</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.804000</td>\n    </tr>\n    <tr>\n      <th>fscore</th>\n      <td>0.828436</td>\n    </tr>\n    <tr>\n      <th>accuracy</th>\n      <td>0.804000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             scores\nprecision  0.886983\nrecall     0.804000\nfscore     0.828436\naccuracy   0.804000"
     },
     "execution_count": 5297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple metrics\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, y_pred_for_test, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_for_test)\n",
    "\n",
    "result_metrics = list()\n",
    "result_metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                        'fscore':fscore, 'accuracy':accuracy}, \n",
    "                        name='scores'))\n",
    "\n",
    "result_metrics = pd.concat(result_metrics, axis=1)\n",
    "\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1030  258]\n [  36  176]]\n"
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data which are predicted as true\n",
    "\n",
    "pred_true_indices = [i for i, y in enumerate(y_pred_for_test) if y == 1]\n",
    "pred_true_indices = pd.Index(pred_true_indices)\n",
    "\n",
    "X_test_pred_true = X_test.copy().reset_index()\n",
    "X_test_pred_true.drop(['index'], axis=1, inplace=True)\n",
    "X_test_pred_true = X_test_pred_true.loc[pred_true_indices]\n",
    "\n",
    "y_test_pred_true = y_test.copy().reset_index()\n",
    "y_test_pred_true.drop(['index'], axis=1, inplace=True)\n",
    "y_test_pred_true = y_test_pred_true.loc[pred_true_indices].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7442396313364056"
     },
     "execution_count": 5300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the data which are predicted as true using stage2 knn model \n",
    "\n",
    "y_pred_pred_true = knn_stage2.predict(X_test_pred_true)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "accuracy = metrics.accuracy_score(y_test_pred_true, y_pred_pred_true)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the final predicted data\n",
    "\n",
    "iter = 0\n",
    "for pred_true_index in pred_true_indices:\n",
    "    y_pred_for_test[pred_true_index] = y_pred_pred_true[iter]\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.894782</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.902000</td>\n    </tr>\n    <tr>\n      <th>fscore</th>\n      <td>0.896769</td>\n    </tr>\n    <tr>\n      <th>accuracy</th>\n      <td>0.902000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             scores\nprecision  0.894782\nrecall     0.902000\nfscore     0.896769\naccuracy   0.902000"
     },
     "execution_count": 5302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple metrics\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test, y_pred_for_test, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_for_test)\n",
    "\n",
    "result_metrics = list()\n",
    "result_metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                        'fscore':fscore, 'accuracy':accuracy}, \n",
    "                        name='scores'))\n",
    "\n",
    "result_metrics = pd.concat(result_metrics, axis=1)\n",
    "\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "precision    recall  f1-score   support\n\n       false       0.93      0.96      0.94      1288\n        true       0.70      0.54      0.61       212\n\n    accuracy                           0.90      1500\n   macro avg       0.81      0.75      0.78      1500\nweighted avg       0.89      0.90      0.90      1500\n\n"
    }
   ],
   "source": [
    "# detail metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_for_test, target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1238   50]\n [  97  115]]\n"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred_for_test))"
   ]
  }
 ]
}